{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.rcParams[\"savefig.dpi\"] = 300\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems with accuracy and unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(values, xlabel=\"predicted labels\", ylabel=\"true labels\", xticklabels=None,\n",
    "                          yticklabels=None, cmap=None, vmin=None, vmax=None, ax=None,\n",
    "                          fmt=\"{:.2f}\", xtickrotation=45, norm=None):\n",
    "    \"\"\"Plot a matrix as heatmap with explicit numbers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : ndarray\n",
    "        Two-dimensional array to visualize.\n",
    "    xlabel : string, default=\"\"\n",
    "        Label for the x-axis.\n",
    "    ylabel : string, default=\"\"\n",
    "        Label for the y-axis.\n",
    "    xticklabels : list of string or None, default=None\n",
    "        Tick labels for the x-axis.\n",
    "    yticklabels : list of string or None, default=None\n",
    "        Tick labels for the y-axis\n",
    "    cmap : string or colormap\n",
    "        Matpotlib colormap to use.\n",
    "    vmin : int, float or None\n",
    "        Minimum clipping value.\n",
    "    vmax : int, float or None\n",
    "        Maximum clipping value.\n",
    "    ax : axes object or None\n",
    "        Matplotlib axes object to plot into. If None, the current axes are\n",
    "        used.\n",
    "    fmt : string, default=\"{:.2f}\"\n",
    "        Format string to convert value to text.\n",
    "    xtickrotation : float, default=45\n",
    "        Rotation of the xticklabels.\n",
    "    norm : matplotlib normalizer\n",
    "        Normalizer passed to pcolor\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    img = ax.pcolormesh(values, cmap=cmap, vmin=vmin, vmax=vmax, norm=norm)\n",
    "    # this will allow us to access the pixel values:\n",
    "    img.update_scalarmappable()\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "    ax.set_xlim(0, values.shape[1])\n",
    "    ax.set_ylim(0, values.shape[0])\n",
    "\n",
    "    if xticklabels is None:\n",
    "        xticklabels = [\"\"] * values.shape[1]\n",
    "    if yticklabels is None:\n",
    "        yticklabels = [\"\"] * values.shape[0]\n",
    "\n",
    "    # +.5 makes the ticks centered on the pixels\n",
    "    ax.set_xticks(np.arange(values.shape[1]) + .5)\n",
    "    ax.set_xticklabels(xticklabels, ha=\"center\", rotation=xtickrotation)\n",
    "    ax.set_yticks(np.arange(values.shape[0]) + .5)\n",
    "    ax.set_yticklabels(yticklabels, va=\"center\")\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    for p, color, value in zip(img.get_paths(), img.get_facecolors(),\n",
    "                               img.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.mean(color[:3]) > 0.5:\n",
    "            # pixel bright: use black for number\n",
    "            c = 'k'\n",
    "        else:\n",
    "            c = 'w'\n",
    "        ax.text(x, y, fmt.format(value), color=c, ha=\"center\", va=\"center\")\n",
    "    ax.invert_yaxis()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, stratify=data.target, random_state=0)\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.zeros(100, dtype=int)\n",
    "y_true[:10] = 1\n",
    "y_pred_1 = np.zeros(100, dtype=int)\n",
    "y_pred_2 = y_true.copy()\n",
    "y_pred_2[10:20] = 1\n",
    "y_pred_3 = y_true.copy()\n",
    "y_pred_3[5:15] = 1 - y_pred_3[5:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for y_pred in [y_pred_1, y_pred_2, y_pred_3]:\n",
    "    print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3)\n",
    "for i, (ax, y_pred) in enumerate(zip(axes, [y_pred_1, y_pred_2, y_pred_3])):\n",
    "    plot_confusion_matrix(confusion_matrix(y_true, y_pred), cmap='gray_r', ax=ax,\n",
    "                          xticklabels=[\"P\", \"N\"], yticklabels=[\"P\", \"N\"], xtickrotation=0, vmin=0, vmax=100)\n",
    "    ax.set_title(\"y_pred_{}\".format(i + 1))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, dpi=100, figsize=(3, 6))\n",
    "\n",
    "for i, (ax, y_pred) in enumerate(zip(axes, [y_pred_1, y_pred_2, y_pred_3])):\n",
    "\n",
    "    plot_confusion_matrix(confusion_matrix(y_true, y_pred), cmap='gray_r', ax=ax,\n",
    "\n",
    "                          xticklabels=[\"P\", \"N\"], yticklabels=[\"P\", \"N\"], xtickrotation=0, vmin=0, vmax=100)\n",
    "\n",
    "    ax.set_title(\"y_pred_{}\".format(i + 1))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "for y_pred in [y_pred_1, y_pred_2, y_pred_3]:\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regresson on breast cancer, but change threshold:\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, stratify=data.target, random_state=0)\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict_proba(X_test)[:, 1] > .85\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will look different on master, I'm using a branch with a bugfix!\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "X, y = make_blobs(n_samples=(2500, 500), cluster_std=[7.0, 2],\n",
    "                  random_state=22)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_test, svc.decision_function(X_test))\n",
    "# find threshold closest to zero:\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(recall[close_zero], precision[close_zero], 'o', markersize=10,\n",
    "         label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n",
    "\n",
    "plt.plot(recall, precision, label=\"precision recall curve\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"images/precision_recall_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# RandomForestClassifier has predict_proba, but not decision_function\n",
    "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(\n",
    "    y_test, rf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.plot(recall, precision, label=\"svc\")\n",
    "\n",
    "plt.plot(recall[close_zero], precision[close_zero], 'o', markersize=10,\n",
    "         label=\"threshold zero svc\", fillstyle=\"none\", c='k', mew=2)\n",
    "\n",
    "plt.plot(recall_rf, precision_rf, label=\"rf\")\n",
    "\n",
    "close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\n",
    "plt.plot(recall_rf[close_default_rf], precision_rf[close_default_rf], '^', c='k',\n",
    "         markersize=10, label=\"threshold 0.5 rf\", fillstyle=\"none\", mew=2)\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"images/rf_vs_svc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 vs average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"f1_score of random forest: {:.3f}\".format(\n",
    "      f1_score(y_test, rf.predict(X_test))))\n",
    "\n",
    "print(\"f1_score of svc: {:.3f}\".format(f1_score(y_test, svc.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "ap_rf = average_precision_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "ap_svc = average_precision_score(y_test, svc.decision_function(X_test))\n",
    "print(\"Average precision of random forest: {:.3f}\".format(ap_rf))\n",
    "print(\"Average precision of svc: {:.3f}\".format(ap_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve SVC\")\n",
    "plt.plot(fpr_rf, tpr_rf, label=\"ROC Curve RF\")\n",
    "\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n",
    "         label=\"threshold zero SVC\", fillstyle=\"none\", c='k', mew=2)\n",
    "close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\n",
    "plt.plot(fpr_rf[close_default_rf], tpr[close_default_rf], '^', markersize=10,\n",
    "         label=\"threshold 0.5 RF\", fillstyle=\"none\", c='k', mew=2)\n",
    "\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "digits = load_digits()\n",
    " # data is between 0 and 16\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data / 16., digits.target, random_state=0)\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "pred = lr.predict(X_test)\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "recall_score(y_test, pred, average='micro'), recall_score(y_test, pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=(2500, 500), cluster_std=[7.0, 2],\n",
    "                  random_state=22)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# default scoring for classification is accuracy\n",
    "scores_default = cross_val_score(SVC(), X, y)\n",
    "\n",
    "# providing scoring=\"accuracy\" doesn't change the results\n",
    "explicit_accuracy =  cross_val_score(SVC(), X, y, scoring=\"accuracy\")\n",
    "\n",
    "# using ROC AUC\n",
    "roc_auc =  cross_val_score(SVC(), X, y, scoring=\"roc_auc\")\n",
    "\n",
    "print(\"Default scoring: {}\".format(scores_default))\n",
    "print(\"Explicit accuracy scoring: {}\".format(explicit_accuracy))\n",
    "print(\"AUC scoring: {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import SCORERS\n",
    "print(\"\\n\".join(sorted(SCORERS.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_support_vectors(est, X, y):\n",
    "    acc = est.score(X, y)\n",
    "    frac_sv = len(est.support_) / np.max(est.support_)\n",
    "    # I just made this up, don't actually use this\n",
    "    return acc / frac_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': np.logspace(-3, 2, 6),\n",
    "              'gamma': np.logspace(-3, 2, 6) / X_train.shape[0]}\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test))\n",
    "print(len(grid.best_estimator_.support_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': np.logspace(-3, 2, 6),\n",
    "              'gamma': np.logspace(-3, 2, 6) / X_train.shape[0]}\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=10, scoring=few_support_vectors)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print((grid.predict(X_test) == y_test).mean())\n",
    "print(len(grid.best_estimator_.support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target)\n",
    "\n",
    "ridge = Ridge(normalize=True).fit(X_train, y_train)\n",
    "pred = ridge.predict(X_test)\n",
    "plt.plot(pred, y_test, 'o', alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.plot([10, 50], [10, 50], '--', c='k')\n",
    "plt.plot(pred, y_test, 'o', alpha=.5)\n",
    "plt.xlabel(\"predicted\")\n",
    "plt.ylabel(\"true\")\n",
    "plt.savefig(\"images/regression_boston.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.plot([0, 50], [0, 0], '--', c='k')\n",
    "plt.plot(y_test, y_test - pred, 'o', alpha=.5)\n",
    "plt.xlabel(\"true\")\n",
    "plt.ylabel(\"true - predicted\")\n",
    "plt.savefig(\"images/regression_boston_2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test - pred, bins=\"auto\");\n",
    "plt.xlabel(\"y_test - pred\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.savefig(\"images/regression_boston_3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    if i > 12:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "    ax.plot(X_train[:, i], y_train, 'o', alpha=.5)\n",
    "    ax.set_title(\"{}: {}\".format(i, boston.feature_names[i]))\n",
    "    ax.set_ylabel(\"MEDV\")\n",
    "plt.savefig(\"images/target_vs_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(y_test - pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    if i > 12:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "    ax.plot(X_test[:, i], y_test - pred, 'o', alpha=.5)\n",
    "    ax.set_title(\"{}: {}\".format(i, boston.feature_names[i]))\n",
    "    ax.set_ylabel(\"y_test - pred\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/residual_vs_feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    return 100 * np.abs((y_true - y_pred) / y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test[:, -1], y_test, 'o', alpha=.5, label=\"true\")\n",
    "plt.plot(X_test[:, -1], pred, 'o', alpha=.5, label='predicted')\n",
    "plt.ylabel(\"MEDV\")\n",
    "plt.xlabel(\"LSTAT\")\n",
    "inds = np.argsort(X_test[:, -1])\n",
    "last = inds[-1]\n",
    "other = inds[1]\n",
    "\n",
    "plt.text(27, 10, \"MAPE={:.2f}\".format(mape(y_test[last], pred[last])))\n",
    "plt.errorbar(X_test[last, -1], y_test[last], fmt='none', yerr=([y_test[last] - pred[last]], [0]), c='k')\n",
    "plt.text(3.5, 40, \"MAPE={:.2f}\".format(mape(y_test[other], pred[other])))\n",
    "plt.errorbar(X_test[other, -1], y_test[other], fmt='none', yerr=([y_test[other] - pred[other]], [0]), c='k')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"images/mape.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[inds[-1]], pred[inds[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test[inds[-1]], pred[inds[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[inds[1]], pred[inds[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test[inds[1]], pred[inds[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
